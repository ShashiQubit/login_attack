# -*- coding: utf-8 -*-
"""syc_count.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/118ycMRDtH3aORUHFUWUneVMNvfuR3YYc

#START
"""

import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import norm, wasserstein_distance, probplot
from scipy.special import lambertw
from statsmodels.graphics import tsaplots
import statsmodels.api as sm
import time
import random
import tensorflow as tf
import tensorflow_probability as tfp

import pandas as pd

import pandas as pd
import numpy as np
import tensorflow as tf

from sklearn.preprocessing import MinMaxScaler
from sklearn.manifold import TSNE

# Load the failed_logins data
failed_logins_data = pd.read_csv("failed_logins_first_500.csv", index_col='time_index')

# Extract the 'failed_logins' column as a numpy array
counts = failed_logins_data['failed_logins'].values.astype(int)

# === Option A Preprocessing (invertible) ===
# log1p transform
y = np.log1p(counts).astype(float)

# Min-Max scaling
scaler = MinMaxScaler(feature_range=(-1,1)).fit(y.reshape(-1,1))
scaled_failed_logins = scaler.transform(y.reshape(-1,1)).flatten()

# Convert to TensorFlow tensor
scaled = tf.convert_to_tensor(scaled_failed_logins)
real_data_flat = scaled

print("Scaled failed_logins series range:", tf.reduce_min(scaled), tf.reduce_max(scaled))


# WGAN-GP (corrected, ready-to-run)
# - Corrects Conv1D input shapes (adds channel dim)
# - Computes gradient penalty per-sample and averages it (correct math)
# - Uses sample-based Wasserstein (wasserstein_distance on raw samples)
# - Stabilizes training loop: uses batched dataset iteration
# - Ensures generator output shape matches critic input
# - Uses tf.float32 for typical TF performance/stability
#
# Usage:
#   - Prepare `gan_data` as a tf.data.Dataset of shape (n_windows, window_size)
#     with dtype tf.float32 (values already scaled).
#   - Call gan.train_wgan_gp(gan_data, preprocessed_data_array, num_epochs, ...).
#
# NOTE: This file focuses on the model class and essential training procedure.
#       Minor I/O, checkpointing, and advanced logging retained but simplified.

import numpy as np
import tensorflow as tf
from scipy.stats import wasserstein_distance

tf.random.set_seed(0)
np.random.seed(0)


class WGAN_GP(tf.keras.Model):
    def __init__(self, window_size, latent_dim=8,
                 critic_filters=(64, 128, 128),
                 gen_dense_units=(30, 50, 50, 50),
                 n_critic=5, gp_weight=10.0):
        """
        window_size: length of 1D sequence window
        latent_dim: dimensionality of generator input noise
        critic_filters: conv filters for critic
        gen_dense_units: dense units for generator (last dense -> window_size)
        n_critic: critic updates per generator update
        gp_weight: gradient penalty coefficient (lambda)
        """
        super().__init__()
        self.window_size = int(window_size)
        self.latent_dim = int(latent_dim)
        self.n_critic = int(n_critic)
        self.gp_weight = float(gp_weight)

        # Build networks
        self.critic = self._build_critic(window_size, critic_filters)
        self.generator = self._build_generator(window_size, latent_dim, gen_dense_units)

        # Metrics history
        self.critic_loss_history = []
        self.generator_loss_history = []
        self.emd_history = []

    def _build_critic(self, window_length, filters):
        """
        Critic expects input shape (batch, window_length, 1).
        Uses Conv1D feature extractor, Flatten, Dense -> scalar score.
        """
        model = tf.keras.Sequential(name="critic")
        model.add(tf.keras.layers.InputLayer(input_shape=(window_length, 1), dtype=tf.float32))
        for f in filters:
            model.add(tf.keras.layers.Conv1D(filters=f, kernel_size=7, padding="same"))
            model.add(tf.keras.layers.LeakyReLU(alpha=0.2))
        model.add(tf.keras.layers.Flatten())
        model.add(tf.keras.layers.Dense(64))
        model.add(tf.keras.layers.LeakyReLU(alpha=0.2))
        model.add(tf.keras.layers.Dense(1))  # output scalar score
        return model

    def _build_generator(self, window_length, latent_dim, dense_units):
        """
        Generator maps latent vector -> (window_length,) and we reshape to (window_length,1).
        Use Dense stack and final Dense(window_length).
        """
        model = tf.keras.Sequential(name="generator")
        model.add(tf.keras.layers.InputLayer(input_shape=(latent_dim,), dtype=tf.float32))
        for u in dense_units:
            model.add(tf.keras.layers.Dense(u))
            model.add(tf.keras.layers.LeakyReLU(alpha=0.2))
        # Final linear output: one value per time step
        model.add(tf.keras.layers.Dense(window_length))
        # Note: no activation so generator learns full range in scaled domain
        return model

    def compile(self, c_optimizer, g_optimizer):
        """
        Store optimizers. Do not call super().compile() to avoid Keras-fit API conflicts.
        """
        self.c_optimizer = c_optimizer
        self.g_optimizer = g_optimizer

    @staticmethod
    def _gradient_penalty(critic, real_samples, fake_samples):
        """
        Compute gradient penalty for a batch of real and fake samples.
        real_samples, fake_samples: tensors shape (batch, window, 1)
        Returns scalar tensor = mean((||grad||_2 - 1)^2)
        """
        batch_size = tf.shape(real_samples)[0]
        # Sample uniform eps for each example in batch -> shape (batch, 1, 1)
        eps = tf.random.uniform(shape=(batch_size, 1, 1), minval=0.0, maxval=1.0, dtype=tf.float32)
        interpolated = eps * real_samples + (1.0 - eps) * fake_samples
        with tf.GradientTape() as gp_tape:
            gp_tape.watch(interpolated)
            # critic output shape: (batch, 1)
            pred = critic(interpolated, training=True)
        # gradients shape: same as interpolated (batch, window, 1)
        grads = gp_tape.gradient(pred, interpolated)
        # flatten per-sample and compute l2 norm per sample
        grads = tf.reshape(grads, (batch_size, -1))
        grads_norms = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=1) + 1e-12)
        gp = tf.reduce_mean((grads_norms - 1.0) ** 2)
        return gp

    @staticmethod
    def distribution_distance(real_samples, fake_samples, n_subsample=10000):
        """
        Sample-based 1D Wasserstein-1 distance (Earth Mover's).
        real_samples, fake_samples: 1D numpy arrays or 1D tensors (flattened values in same domain)
        n_subsample: max number of points to use (subsample larger arrays for speed)
        """
        r = np.asarray(real_samples).ravel()
        f = np.asarray(fake_samples).ravel()
        # subsample uniformly if arrays are large
        if r.size > n_subsample:
            idx = np.random.choice(r.size, size=n_subsample, replace=False)
            r = r[idx]
        if f.size > n_subsample:
            idx = np.random.choice(f.size, size=n_subsample, replace=False)
            f = f[idx]
        # compute sample-based Wasserstein
        return float(wasserstein_distance(r, f))

    def train_wgan_gp(self, gan_dataset, preprocessed_data_flat,
                      epochs=100, batch_size=32, eval_num_samples=10000,
                      verbose=True, checkpoint_every=50, checkpoint_prefix=None):
        """
        Training loop for WGAN-GP.

        gan_dataset: tf.data.Dataset of 1D windows shape (window,) dtype float32
        preprocessed_data_flat: 1D numpy array or tensor with the same scaled values used to build gan_dataset
                              (used for evaluation/EMD). It should be flattened (no windows).
        epochs: number of epochs to train
        batch_size: critic/generator minibatch size
        eval_num_samples: number of generated samples to use for EMD evaluation (flattened after reshaping)
        checkpoint_every: save weights every N epochs if checkpoint_prefix provided (optional)
        """
        # Ensure dataset is batched and prefetch for performance
        dataset = gan_dataset.shuffle(buffer_size=10000).batch(batch_size).prefetch(tf.data.AUTOTUNE)

        # ensure preprocessed_data_flat is numpy array for EMD computation
        if isinstance(preprocessed_data_flat, tf.Tensor):
            preprocessed_vals = preprocessed_data_flat.numpy().ravel()
        else:
            preprocessed_vals = np.asarray(preprocessed_data_flat).ravel()

        for epoch in range(1, epochs + 1):
            # ---------------------
            # Train critic n_critic times per epoch
            # ---------------------
            c_losses = []
            # Create an iterator so we can call next() repeatedly
            data_iter = iter(dataset)
            for _ in range(self.n_critic):
                try:
                    real_batch = next(data_iter)  # shape (batch, window)
                except StopIteration:
                    # re-create iterator if exhausted
                    data_iter = iter(dataset)
                    real_batch = next(data_iter)
                # ensure float32 and add channel dim
                real_batch = tf.cast(real_batch, tf.float32)
                real_batch = tf.reshape(real_batch, (-1, self.window_size, 1))

                # sample latent noise and produce fake batch
                noise = tf.random.normal(shape=(tf.shape(real_batch)[0], self.latent_dim), dtype=tf.float32)
                fake_batch = self.generator(noise, training=True)  # shape (batch, window)
                fake_batch = tf.reshape(fake_batch, (-1, self.window_size, 1))

                with tf.GradientTape() as tape:
                    # critic scores
                    real_score = self.critic(real_batch, training=True)  # (batch, 1)
                    fake_score = self.critic(fake_batch, training=True)  # (batch, 1)
                    # gradient penalty
                    gp = self._gradient_penalty(self.critic, real_batch, fake_batch)
                    # WGAN-GP critic loss (mean over batch)
                    c_loss = tf.reduce_mean(fake_score) - tf.reduce_mean(real_score) + self.gp_weight * gp

                grads = tape.gradient(c_loss, self.critic.trainable_variables)
                self.c_optimizer.apply_gradients(zip(grads, self.critic.trainable_variables))
                c_losses.append(float(c_loss.numpy()))

            # ---------------------
            # Train generator once
            # ---------------------
            # sample noise
            z = tf.random.normal(shape=(batch_size, self.latent_dim), dtype=tf.float32)
            with tf.GradientTape() as gen_tape:
                generated = self.generator(z, training=True)          # (batch, window)
                generated_reshaped = tf.reshape(generated, (-1, self.window_size, 1))
                # negative critic score as generator loss (maximize critic(fake))
                gen_score = self.critic(generated_reshaped, training=True)
                g_loss = -tf.reduce_mean(gen_score)

            gen_grads = gen_tape.gradient(g_loss, self.generator.trainable_variables)
            self.g_optimizer.apply_gradients(zip(gen_grads, self.generator.trainable_variables))

            # record metrics
            self.critic_loss_history.append(np.mean(c_losses))
            self.generator_loss_history.append(float(g_loss.numpy()))

            # ---------------------
            # Evaluate EMD on samples (sample-based, stable)
            # ---------------------
            # generate a sufficient number of samples, flatten them and compare to preprocessed data
            n_blocks = max(1, eval_num_samples // self.window_size)  # number of generator calls
            # collect generated blocks
            gen_collected = []
            for _ in range(n_blocks):
                z_eval = tf.random.normal(shape=(batch_size, self.latent_dim), dtype=tf.float32)
                g_eval = self.generator(z_eval, training=False).numpy()  # shape (batch, window)
                gen_collected.append(g_eval.reshape(-1))  # flatten per batch
            gen_all = np.concatenate(gen_collected, axis=0)
            # clip to comparable length to preprocessed values if desired
            # subsample real to match gen_all length for fair comparison
            n_comp = min(len(preprocessed_vals), len(gen_all))
            real_sub = np.random.choice(preprocessed_vals, size=n_comp, replace=False)
            fake_sub = np.random.choice(gen_all, size=n_comp, replace=False)
            emd_val = self.distribution_distance(real_sub, fake_sub, n_subsample=10000)
            self.emd_history.append(emd_val)

            # Optional checkpoint
            if checkpoint_prefix and (epoch % checkpoint_every == 0):
                self.generator.save_weights(f"{checkpoint_prefix}_generator_epoch_{epoch}.h5")
                self.critic.save_weights(f"{checkpoint_prefix}_critic_epoch_{epoch}.h5")

            # Verbose logging
            if verbose and (epoch % max(1, epochs // 20) == 0 or epoch == 1):
                print(f"Epoch {epoch}/{epochs} | c_loss: {self.critic_loss_history[-1]:.6f} | "
                      f"g_loss: {self.generator_loss_history[-1]:.6f} | emd: {self.emd_history[-1]:.6f}")

        # End of epochs
        return {
            "critic_loss": np.array(self.critic_loss_history),
            "generator_loss": np.array(self.generator_loss_history),
            "emd": np.array(self.emd_history)
        }

# -------------------------
# Minimal example usage
# -------------------------
# # Prepare your data: y_scaled is 1D numpy array of scaled values from log1p + MinMax
window = 30
from numpy.lib.stride_tricks import sliding_window_view
windows = sliding_window_view(scaled, window_shape=window)
gan_dataset = tf.data.Dataset.from_tensor_slices(windows.astype(np.float32))

model = WGAN_GP(window_size=window, latent_dim=8, n_critic=5, gp_weight=10.0)
model.compile(c_optimizer=tf.keras.optimizers.Adam(1e-4),
              g_optimizer=tf.keras.optimizers.Adam(1e-4))
history = model.train_wgan_gp(gan_dataset, preprocessed_data_flat=y_scaled,
                           epochs=2, batch_size=32, eval_num_samples=20000,
                            verbose=True, checkpoint_prefix="checkpoints/wgan")



# ============================
# Plot training history
# ============================

def plot_training_history(history, window=50, savepath=None):
    """
    history: dict returned by train_wgan_gp containing
             'critic_loss', 'generator_loss', 'emd'
    window: moving average window size
    savepath: optional path to save plot
    """
    critic_loss = np.array(history["critic_loss"])
    generator_loss = np.array(history["generator_loss"])
    emd = np.array(history["emd"])

    def moving_average(x, w):
        if len(x) < w: return x
        return np.convolve(x, np.ones(w)/w, mode='valid')

    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # Losses
    axes[0].plot(critic_loss, color='black', alpha=0.3, label='Critic Loss')
    axes[0].plot(moving_average(critic_loss, window), color='blue', label='Critic MA')
    axes[0].plot(generator_loss, color='gray', alpha=0.3, label='Generator Loss')
    axes[0].plot(moving_average(generator_loss, window), color='orange', label='Generator MA')
    axes[0].set_title("WGAN Losses")
    axes[0].set_ylabel("Loss")
    axes[0].set_xlabel("Epoch")
    axes[0].grid(True)
    axes[0].legend()

    # EMD
    axes[1].plot(emd, color='red', alpha=0.5, label='EMD')
    axes[1].plot(moving_average(emd, window), color='darkred', linewidth=2, label='EMD MA')
    axes[1].set_title("Earth Mover’s Distance (Real vs Fake)")
    axes[1].set_ylabel("EMD")
    axes[1].set_xlabel("Epoch")
    axes[1].grid(True)
    axes[1].legend()

    plt.tight_layout()
    if savepath:
        plt.savefig(savepath, dpi=300)
    plt.show()


# ============================
# Post-processing and inference
# ============================

def load_generator_checkpoint(generator, checkpoint_path):
    """
    Load generator weights from a saved checkpoint.
    """
    generator.load_weights(checkpoint_path)
    print(f"Loaded generator weights from {checkpoint_path}")


def generate_synthetic_series(generator, latent_dim, total_length, window_size=30):
    """
    Generate synthetic time series by sampling windows from generator and concatenating.
    """
    n_blocks = int(np.ceil(total_length / window_size))
    series = []
    for _ in range(n_blocks):
        z = tf.random.normal(shape=(1, latent_dim))
        g_sample = generator(z, training=False).numpy().flatten()
        series.append(g_sample)
    synthetic = np.concatenate(series)[:total_length]
    return synthetic


def tsne_comparison(real_series, fake_series, n_points=2000, perplexity=30, random_state=0, savepath=None):
    """
    Perform TSNE comparison of real and fake samples.
    """
    # Subsample to equal lengths
    n = min(len(real_series), len(fake_series), n_points)
    idx_real = np.random.choice(len(real_series), n, replace=False)
    idx_fake = np.random.choice(len(fake_series), n, replace=False)

    real_sub = real_series[idx_real].reshape(-1, 1)
    fake_sub = fake_series[idx_fake].reshape(-1, 1)

    combined = np.vstack([real_sub, fake_sub])
    labels = np.array([0]*n + [1]*n)  # 0=real, 1=fake

    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=random_state)
    embeddings = tsne.fit_transform(combined)

    plt.figure(figsize=(8, 6))
    plt.scatter(embeddings[labels==0, 0], embeddings[labels==0, 1],
                alpha=0.6, label="Real", s=10)
    plt.scatter(embeddings[labels==1, 0], embeddings[labels==1, 1],
                alpha=0.6, label="Synthetic", s=10)
    plt.legend()
    plt.title("t-SNE Projection: Real vs Synthetic")
    if savepath:
        plt.savefig(savepath, dpi=300)
    plt.show()


# ============================
# Example usage after training
# ============================

# Suppose:
#   - model is your WGAN_GP instance
#   - history = model.train_wgan_gp(...)
#   - real_data_flat = your scaled real series (1D numpy array)

# 1. Plot training history
plot_training_history(history, window=50, savepath="training_history.png")

# 2. Reload generator from last checkpoint
load_generator_checkpoint(model.generator, "checkpoints/wgan_generator_epoch_200.h5")

# 3. Generate synthetic series

synthetic = generate_synthetic_series(model.generator, latent_dim=8,
                                       total_length=len(real_data_flat),
                                      window_size=model.window_size)

# 4. Compare real vs synthetic using t-SNE
tsne_comparison(real_data_flat, synthetic, n_points=2000,
                 savepath="tsne_comparison.png")

