# -*- coding: utf-8 -*-
"""WGAN_iio.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z970v1oodUOREzs_yI3hNtP3zf7ayg3z

# Generative Modeling and Financial Time Series

### A Brief Introduction in Economy

Since the first societies in ancient Greece, it was imperative to design a system by which the society produces, distributes and consumes goods and services, which is known as *economy*. The economy involves the production, trade and consumption of goods and services by individuals, businesses and governments and can be divided into various sectors such as agriculture, industry, real estate and services. The overall health of an economy is typically measured by various economic indicators such as *gross domestic product (GDP)*, *employment rates*, *inflation rates* and *consumer confidence*. The performance of an economy can be influenced by different factors such as government policies, technological advancements, natural disasters and global events (e.g the Covid pandemic).

#### The iio Index

The iio index is a market capitalization-weighted index of 500 large-cap stocks traded on the two largest U.S. stock exchanges: the New York Stock Exchange (NYSE) and the Nasdaq Stock Market. These stocks are selected based on certain criteria such as market capitalization, liquidity, and financial viability and are representative of the U.S. economy. It is widely regarded as one of the best measures of the performance of the US equities market and it is often used as a benchmark for the overall performance of the US stock market. It was created by Standard & Poor's in 1957 and includes companies from various industries. The iio index is frequently used as a gauge of the health of the US economy and is tracked by investors, analysts and financial media around the world.

We are going to make use of this index in order to study some aspects of financial time series, as well as develop a *generative adversarial network (GAN)* that generates synthetic data that resembles the iio index. Then, we will extend this idea to the quantum computing field, where we will implement a hybrid classical-quantum *variational model* with a *parametrized quantum circuit (PQC)* as the generator of a higher-level framework known as *quantum GAN (qGAN)*,

### Financial Time Series

Economists study the functioning of economies and develop theories to explain and predict economic phenomena. Understanding the economy is important because it can affect the well-being of individuals and societies as a whole, including their standard of living, income, employment opportunities and access to goods and services. One of these theories is a mathematical framework which we refer to as *financial time series*.

A time series is generally a set of points indicating some quantity over discrete steps in time. This may include temperatures, data produced by sensors or some other value. The iio daily closing price values over a period of time is such an example. When we are dealing with finance, we call this model a financial time series.

Let us load the iio daily closing values from 1/1/2000 since 31/12/2022, using the *yfinance* module in Python which provides a more direct interface to the Yahoo Finance API and visualize the graph:
"""

import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import norm, wasserstein_distance, probplot
from scipy.special import lambertw
from statsmodels.graphics import tsaplots
import statsmodels.api as sm
import time
import random
import tensorflow as tf
import tensorflow_probability as tfp

import pandas as pd



# limit GPU memory growth to avoid OUT_OF_MEMORY issues (before initializing GPU -- if used)


gpus = tf.config.experimental.list_physical_devices('GPU')


if gpus:
    try:
        tf.config.experimental.set_memory_growth(gpus[0], True)
    except RuntimeError as e:
        print(e)



# Load the data from the saved CSV file with a standard single-level header

iio = pd.read_csv('iio.csv', parse_dates=['timestamp'])
iio.sort_values('timestamp', inplace=True)
iio.drop_duplicates(subset='timestamp', inplace=True)
iio['value'] = pd.to_numeric(iio['value'], errors='coerce')
iio.dropna(subset=['value'], inplace=True)
# optional resample to daily: iio = iio.set_index('timestamp').resample('D').mean().dropna().reset_index()
start_date = '2013-10-11'
iio_sliced = iio[iio['timestamp'] >= start_date].copy()


print(f"Sliced total rows: {len(iio_sliced)}")
print(f"Sliced date range: {iio_sliced['timestamp'].min()} to {iio_sliced['timestamp'].max()}")
print("-" * 40)


# The rest of your code now operates on the SLICED data
iio_values = iio_sliced['value'].values.astype(np.float64)
date = iio_sliced['timestamp']

# Convert the numeric 'value' column to a TensorFlow tensor
iio_close = tf.convert_to_tensor(iio_values, dtype=tf.float64)

# Display the shape of the final tensor
# This shape will now reflect the number of rows AFTER the slice
print('Shape of final tensor (iio_close): ', tf.shape(iio_close))



# check if GPU is available

print("GPU Available:", tf.config.list_physical_devices('GPU'))


# check which GPU is being used

print("GPU Device:", tf.test.gpu_device_name())


# Use the DataFrame's index for the date information

#date = iio.index


# plot the iio data
fig, axes = plt.subplots(1, 1, figsize=(12, 6), sharex=True)

axes.plot(iio_close, color='blue', alpha=0.7)
axes.set_title("Original AWS Input Series")
axes.grid(True)


plt.xlabel('Time')

plt.ylabel('iio Value')

plt.title('iio Index')


#plt.savefig("plots/raw_series_of_iio (WGAN).svg", format="svg")
#plt.close()

import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates

# Set the start date for slicing, same as before
start_date = '2013-10-11'

try:
    # --- 1. Load and process the original data ---
    iio_original = pd.read_csv('iio.csv', parse_dates=['timestamp'])
    iio_original.sort_values('timestamp', inplace=True)
    iio_original.drop_duplicates(subset='timestamp', inplace=True)
    iio_original['value'] = pd.to_numeric(iio_original['value'], errors='coerce')
    iio_original.dropna(subset=['value'], inplace=True)

    # --- 2. Create the sliced data ---
    iio_sliced = iio_original[iio_original['timestamp'] >= start_date].copy()

    print("Data loaded and processed successfully.")
    print(f"Original series starts on: {iio_original['timestamp'].min()}")
    print(f"Sliced series starts on:   {iio_sliced['timestamp'].min()}")

    # --- 3. Create the overlay plot ---
    fig, ax = plt.subplots(figsize=(14, 7))

    # Plot the full original series in a semi-transparent blue
    ax.plot(iio_original['timestamp'], iio_original['value'], label='Original Series', color='skyblue', alpha=0.9)

    # Plot the sliced series on top in a solid, slightly thinner red line
    ax.plot(iio_sliced['timestamp'], iio_sliced['value'], label=f'Sliced Series (from {start_date})', color='red', linewidth=1.5)

    # Add a vertical dashed line to clearly mark the cut-off point
    ax.axvline(pd.to_datetime(start_date), color='black', linestyle='--', linewidth=1.5, label=f'Cut-off: {start_date}')

    # --- 4. Formatting the plot for clarity ---
    ax.set_title('Overlay of Original vs. Sliced Time Series', fontsize=16)
    ax.set_xlabel('Timestamp', fontsize=12)
    ax.set_ylabel('Value', fontsize=12)
    ax.legend(fontsize=10)
    ax.grid(True, which='both', linestyle='--', linewidth=0.5)

    # Improve date formatting on the x-axis for readability
    fig.autofmt_xdate()
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))

    plt.tight_layout()

    # Save the figure to a file
    output_filename = 'overlay_plot.png'
    plt.savefig(output_filename)

    print(f"\nPlot saved successfully as '{output_filename}'")

except FileNotFoundError:
    print("Error: The file 'iio.csv' could not be found.")
except Exception as e:
    print(f"An error occurred: {e}")

len(iio_sliced)

"""The iio index is clearly in an uptrend over the years. This is logical, as it kind of measures the overall performance between 500 most successful companies in the U.S. It's current value is more than 8000% up from the value in the early 60s and 70s.

#### Raw price fluctuations vs Logarithmic Returns

The upward trend is a common property in many financial time series, as economies tend to grow over time, which means that the value of the index may increase over time even if there are fluctuations in the short term. Therefore, studying the index's price alone may not be sufficient to compare the index's performance over different periods of time. Instead, analysts use the *returns* instead of prices for such purposes. The *logarithmic returns* of a stock or an index is defined by the equation

$$ r_t = \ln(p_t) - \ln(p_{t-1}) = \ln \left ( \frac{p_t}{p_{t-1}} \right ) $$

where $r_t$ are the logarithmic returns at time $t$ and $p_t$ is the stock or index price at time $t$. Also, we assume that the prices are equally spaced by some constant $\Delta t$ (in our case, this is one day). For further details, refer to *'A. Chakraborti, B. Chakrabarti, S. Sinha, and A. Chatterjee, Econophysics: An Introduction, Wiley-vch, 2011'*.

The logarithmic returns have some special properties that we wish to exploit, which are not present if we used the direct returns instead, given by $p_t - p_{t-1}$. Let us visualize the direct returns and the logarithmic ones to get an idea:
"""

# direct returns over time
iio_direct_r = iio_close[1:] - iio_close[:-1]

# logarithmic returns over time
iio_log_r = np.log(iio_close[1:]) - np.log(iio_close[:-1])

# plot the graphs side-by-side
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,4))

axes[0].plot(iio_direct_r)
axes[0].set_title('iio Index Direct Returns')
axes[0].grid()

axes[1].plot(iio_log_r)
axes[1].set_title('iio Index Log Returns')
axes[1].grid()

# Show the plot
plt.savefig("plots/iio Index Direct Returns_and_iio Index Log Returns (WGAN).svg", format="svg")
plt.close()

"""##### Scaling over time

The first thing to notice is how the returns scale over time. The log-returns stay within an approximate range, whereas the bounds of the direct returns range are altered. Raw price fluctuations can vary widely depending on the value of the index, which can make it difficult to compare fluctuations at different points in time. However, log-returns account for the percentage change in price, which helps to normalize the data and makes it easier to compare fluctuations across different periods. This is because of the compression that the logarithmic function induces on the values. Additionally, log-returns are additive, which means that the return over a longer period can be calculated by summing the returns over shorter periods, which is not the case with raw price fluctuations. This allows for simple arithmetic operations, such as averaging, which can be useful for various applications.

##### Returns Rate

Also, the *returns rate* $R_t$ at time $t$ is closely related to log-returns as

$$ R_t = \frac{p_t - p_{t-1}}{p_{t-1}} \approx \ln \left ( \frac{p_t}{p_{t-1}} \right ) $$

The returns rate is simply the percentage change in the value of an investment over a specific period of time. Log-returns are calculated using the natural logarithm of the ratio between the final and initial values of an investment. Since the logarithmic function is continuous and differentiable, log-returns can be interpreted as the continuously compounded returns rate. This means that a log-return of 0.01 (or 1%) can be interpreted as a continuously compounded return rate of 1%.

##### Volatility Smile

Log-returns are also related to *stock volatility*, which measures how much the price of a stock fluctuates over time. The relationship between log-returns and stock volatility is known as the *volatility smile*. It is a common feature of financial markets where implied volatility, which is derived from options prices, varies with respect to the strike price and time to expiration. In general, the volatility smile implies that log-returns are not normally distributed, but instead have fatter tails than a normal distribution would suggest. We will come to this right after.

### Stylized Empirical Facts

Over the years, analysts have extracted several statistical properties from financial time series that are present in almost every market, which are known as *stylized facts*. This is essential, as there is no standard procedure of how the values of such a series are produced. As a result, predicting future stock prices is out of the question. As a consequence of many independent empirical studies on the statistical properties of financial markets, there is a wide collection of stylized facts. We will be concerned with the most basic ones.

#### Log-Returns is not Gaussian

In order to see this, we plot the probability density function (PDF) of the log-returns along with a Gaussian distribution with the same mean and standard deviation:
"""

# plot the graphs side-by-side
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,4))

# density of log-returns
bin_edges = np.linspace(-0.05, 0.05, num=50)  # define the bin edges
bin_width = bin_edges[1] - bin_edges[0]
bin_edges = np.append(bin_edges, bin_edges[-1] + bin_width)
axes[0].hist(iio_log_r, bins=bin_edges, density=True, width=0.001, label='Log-returns density')
axes[0].grid()

# normal distribution with same mean and standard deviation as log-returns
mu = np.mean(iio_log_r)
sigma = np.std(iio_log_r)

# Generate a set of points x
x = np.linspace(-0.05, 0.05, 100)

# Generate the Gaussian PDF for the points x with same mean and standard deviation as the log-returns
pdf = norm.pdf(x, mu, sigma)

# plot the Gaussian PDF
axes[0].plot(x, pdf, 'r', label='Gaussian distribution')
axes[0].legend()

# plot in logarithmic scale
axes[1].hist(iio_log_r, bins=bin_edges, density=True, width=0.001, log=True)
axes[1].grid()

# plot the Gaussian PDF in logarithmic scale
axes[1].semilogy(x, pdf, 'r')

plt.savefig("plots/Gaussian_PDF_and_log_returned_density (WGAN).svg", format="svg")
plt.close()

"""On the left, we have the histogram of the log-returns along with a Gaussian distribution with the same parameters and on the right, we plot the same results in logarithmic scale to inspect in further detail. The log-returns density peak is almost doubled with respect to the Gaussian distribution. Also, by inspecting the right figure, we see that the tails of the log-returns do not decay as fast as the Gaussian. This property is known as *fat tails* and shows that volatility is present more often than a Gaussian distribution would predict.

It is worth noting that these two properties are not so well clarified when we increase the time scale from days to weeks or months. This is so-called *aggregational normality*.

#### No Linear Autocorrelation

The previous observation comes from a statistical perspective and is independent of time. However, there are some stylized facts that deal with properties related to time. One such property is the absence of linear autocorrelations. We can inspect this by calculating the *autocorrelation function (ACF)* of the log-returns of the iio for a range of different lags

$$ \rho (\tau) = corr(r_t, r_{t+\tau}) = \frac{cov(r_t, r_{t+\tau})}{\sigma_{r_t} \sigma_{r_{t+\tau}}} $$

where $r_t$, $r_{t+\tau}$ are the returns, $\sigma_{r_{t}}$, $\sigma_{r_{t+\tau}}$  are the standard deviations and $cov(r_t, r_{t+\tau})$ is the covariance of the returns between times $t$ and $t+\tau$, where $\tau$ is the lag. Since we have daily timeframe, $\tau \in \mathbb{N}^*$.

We calculate the autocorrelations of the log-returns at a range of lags $\tau \in \{1,\dots,30\}$, so the maximum lag is 30 days.
"""

# convert the iio log returns to a TensorFlow tensor
iio_log_r_tf = tf.convert_to_tensor(iio_log_r)

# plot the ACF for the specified lags
tsaplots.plot_acf(iio_log_r_tf, lags=30, zero=False)
plt.xlabel('Lags')
plt.title('ACF iio Log-Returns')
plt.ylabel(r'$\rho$')
plt.grid()
plt.savefig("plots/ACF iio Log-Returns (WGAN).svg", format="svg")
plt.close()

"""As we can see, the values are very close to zero, so there is no sufficient autocorrelation. In an efficient market, stock prices are believed to reflect all known information about the market, which means that any new information that becomes available will be quickly incorporated into the stock price. As a result, there should be no predictable pattern of price movements over time and the autocorrelation of the stock returns should be close to zero. This is because, if investors are rational and acting on all available information, the current stock price should reflect the expected future value of the stock and any deviation from that expected value would be quickly corrected as new information becomes available. Therefore, the absence of autocorrelation is actually to be expected in an efficient market, since the current price already incorporates all available information and there should be no predictable pattern of price movements over time. Also, as lags are getting higher, autocorrelation decreases.

#### Volatility Clustering

Volatility clustering is a phenomenon in financial markets where periods of high volatility tend to be followed by other periods of high volatility and periods of low volatility tend to be followed by other periods of low volatility. This means that volatility tends to cluster together in time, rather than being randomly distributed.

We can get an intuitive understanding of volatility clustering by looking at the log-returns. However, we wish to quantify this phenomenon. Specifically, we can do this by looking at the autocorrelation function ACF of the *absolute log-returns* $\left | r_t \right |$ and $\left | r_{t+\tau} \right |$ over a range of lags $\tau$ defined by

$$ \rho_{abs} (\tau) = corr(\left | r_t \right |, \left | r_{t+\tau} \right |) $$
"""

# plot the ACF of absolute log-returns for the specified lags
tsaplots.plot_acf(tf.abs(iio_log_r_tf), lags=30, zero=False)
plt.xlabel('Lags')
plt.title('ACF iio Absolute Log-Returns')
plt.ylabel(r'$\rho_{abs}$')
plt.grid()
plt.savefig("plots/ACF iio Absolute Log-Returns (WGAN).svg", format="svg")
plt.close()

"""The presence of significant autocorrelation for absolute log-returns is an indication that there are patterns in the data that are related to the magnitude of price changes and that these patterns can be characterized by volatility clustering. Also, autocorrelation decreases as the lag is increased. This is expected in efficient markets. This plot provides insights into the persistence of volatility over time in the financial time series.

#### The Leverage Effect

But, how the returns are correlated to volatility ? The *leverage effect* is a stylized fact that deals with this and is commonly present in financial time series. It is given by

$$ L(\tau) = corr \left(r_{t+\tau}^2, r_t\right) $$

where $r_{t+\tau}^2$ is another widely used metric for volatility.

Let us calculate and inspect the leverage effect:
"""

lags = range(1, 31)
lev = []
for lag in lags:
    # slice the tensors to get the appropriate lagged sequences
    r_t = iio_log_r_tf[:-lag]
    squared_lag_r = tf.square(tf.abs(iio_log_r_tf[lag:]))

    # calculate the leverage effect
    lev.append(tfp.stats.correlation(r_t, squared_lag_r, sample_axis=0, event_axis=None))

# plot the the levarage effect
plt.plot(lev)
plt.title('Leverage Effect iio')
plt.xlabel('Lags')
plt.ylabel(r'$L(\tau)$')
plt.grid()
plt.savefig("plots/Leverage Effect iio (WGAN).svg", format="svg")
plt.close()

"""As the lag increases, the leverage effect goes to 0, starting from a lower negative value. The returns are *negatively correlated* to price volatility. Specifically, it means that as the price of an asset falls, the volatility of that asset tends to increase and vice versa. This implies that when the market is experiencing negative returns, the volatility of that market tends to increase.

The reason for the leverage effect is due to the presence of financial leverage, which refers to the use of borrowed funds to invest in assets. When investors use leverage to buy an asset, they increase the risk of their investment since they have to pay back the borrowed funds regardless of the performance of the asset. If the asset performs poorly, then the investor may be forced to sell at a loss to pay back the borrowed funds, which can further amplify the downward price movement and increase the volatility of the asset.

### Generative Modeling: Application in Financial Time Series

Generative modeling deals with generation of synthetic data that closely resemble some other data. It is a form of unsupervised learning where the model at hand is trained to capture the underlying statistics and learn the properties of some dataset. There are several techniques (e.g restricted Boltzmann machines, Deep-Belief networks, GANs) involved for various appications, such as image generation, data augmentation, feature extraction, style transfer and text inference among others. We will look at the promising method of generative adversarial networks (GANs). The general high-level architecture of a GAN is shown below (Source: Google):

<img src="../images/gan.png" width="60%" align="center">

Briefly explained, GANs include two neural networks called the *generator* and *discriminator* respectively. These two networks compete in a *zero-sum game*. In each training round, the discriminator is trained first on a batch of real data and then on a batch of synthetic data that the generator produced, given random noise as input. The discriminator penalizes the generator for not producing synthetic data that is close to the real data via feedback and tries to minimize the generator error. Then, the generator is trained on random noise and based on the feedback of the discriminator, it tries to generate data that closely resemble the real ones by maximizing the discriminator's error. The models are tained consequtively and independently. In an ideal case where the generator produces perfect replicas of the data, the discriminator loss lies to 50%, as it is not able to distinguish between real and fake data. This competitive training continues until the model has adequate performance. The latter is closely related to the task at hand.

The most common GAN architectures used for time series data include LSTM-GANs and WGANs.

It's important to acknowledge the limitations of generative modeling in financial time series. Generating realistic financial data can be challenging due to the complex and dynamic nature of financial markets. The generated samples may not fully capture the intricacies of real-world financial data and caution should be exercised when using generated data for decision-making or risk management purposes.

#### LSTM-GANs
LSTM-GANs utilize a type of neural network called a *Long Short-Term Memory (LSTM)* network, which is a type of *recurrent neural network (RNN)* that can remember past inputs and use that information to inform future predictions. In an LSTM-GAN, the generator is an LSTM network that takes as input a randomly generated noise vector and produces a sequence of synthetic financial time series data. The discriminator is also an LSTM network that takes as input a sequence of either real or synthetic financial time series data and outputs a probability indicating whether the input data is real or synthetic.

#### WGANs
WGANs use a different type of loss function called the *Wasserstein distance*, which measures the distance between two probability distributions. In a WGAN, the generator produces synthetic financial time series data and the discriminator outputs a scalar value representing the probability that a given sequence of financial time series data is real or synthetic. The WGAN training process involves minimizing the Wasserstein distance between the probability distributions of the real and synthetic financial time series data. This is a variant of the GAN framework.

The Wasserstein distance, also known as Earth Mover’s Distance (EMD) is a ***quantitative metric*** that allows the comparison of two distributions that two different datasets follow, without assuming any underlying distribution. Assume that the reference distribution is denoted by $P$ and the model distribution by $Q$. As we are interested with one-dimensional data, we use the so-called Wasserstein-1 distance defined by

$$ W_1 (P,Q) = \int_{-\infty}^{\infty} \left | F_P (x) - F_Q (x) \right | dx $$,

where $F_P(x)$ and $F_Q(x)$ are the cumulative distribution functions of $P$ and $Q$ respectively and $x$ denotes the samples. This needs a numerical method in order to be computed. It can be found in the Python *SciPy* library, built on top of *Numpy*, which is widely used for scientific and technical computing.

##### WGAN with Gradient Penalty (WGAN-GP)
The key difference between WGAN and other GAN variants is the use of a *critic* instead of a discriminator. The critic is trained to output a real number rather than a probability and is optimized to minimize the Wasserstein distance between the real and generated distributions. The WGAN with gradient penalty (WGAN-GP) is an improved version of the original WGAN that introduces a gradient penalty term to the critic's loss function. In the WGAN-GP, the critic is penalized when its gradient norm deviates from 1, which is the desired value for a Lipschitz continuous function. This is achieved by computing the gradient penalty as the squared difference between the norm of the critic's gradient and 1 and adding it to the original Wasserstein distance objective. If we assume that the output of the critic is denoted by $C(x)$, where $x$ is a sample, the critic's loss function becomes

$$ W_1' (P,Q) = \int_{-\infty}^{\infty} \left | F_P (x) - F_Q (x) \right | dx + \left ( \| \nabla_x C \| - 1 \right )^2 $$

This encourages the critic to have a gradient norm close to 1 everywhere, which in turn enforces the Lipschitz continuity constraint and leads to more stable training and improved sample quality. Also, it helps avoid the problem of vanishing gradients and mode collapse. The magnitude of the gradient penalty is controlled by a hyperparameter called the penalty coefficient, which determines the strength of the penalty relative to the original Wasserstein distance objective.

This is the type of GAN we will implement.

##### Kullback-Liebler Divergence vs Wasserstein Distance
As we know, the *Kullback-Leibler (KL) divergence* is a measure of how one probability distribution diverges from a second, reference probability distribution. It is a common loss function used in generative models and one may notice the similarity with the Wasserstein distance. However, when used in GANs, KL divergence can suffer from the problem of *vanishing gradients*, where the gradients become too small to make meaningful updates to the generator network. This is mainly due to the logarithmic function. Also, it can lead to mode collapse, where the generator produces only a few modes of the true data distribution and ignoring the rest. This is because KL divergence penalizes the generator if it generates samples that are far from the true distribution, but it does not provide any reward for generating samples that are different from the ones already generated.

On the other hand, Wasserstein distance can be more stable during training, as it avoids the problem of vanishing gradients that can affect the KL divergence. It has been shown to be effective at reducing mode collapse, producing more diverse samples from the generator. The Wasserstein distance can be interpreted as a measure of how much work (in terms of distance multiplied by probability mass) is needed to transform one distribution into another, providing a useful metric of the difference between the true and generated data distributions. A disadvantage is that it can be computationally expensive to calculate, requiring the use of specialized optimization algorithms such as the *earth-mover's distance (EMD)* or *Sinkhorn distance*. Also, it can be sensitive to the choice of distance metric used to calculate it and it can be difficult to choose an appropriate metric for a given problem.

For further details on WGANs, refer to Arjovsky's et. al *'Wasserstein GAN'*: https://arxiv.org/abs/1701.07875, (2017).

### Model Performance

When dealing with WGANs for financial time series, one should carefully consider what metrics will define the model performance. The generated synthetic data must be close to the original data and should be able to replicate the stylized facts we talked about earlier:
 - The log-returns do not follow a Gaussian distribution
 - The absence of autocorrelation
 - Volatility clustering
 - Leverage Effect

##### Q-Q Plot
The metrics above offer a ***qualitative criterion*** of the model performance. However, we will also need a proper way to compare the underlying distributions of the generated data and the iio log-returns data in order to get another qualitative metric which will enhance model evaluation along with the histograms. For this purpose, we will use a visualization technique called *Q-Q plot* which gives the ability of better inspection of the specific areas of the distribution where the two distributions differ.
"""

# Generate the Q-Q plot
plt.figure(figsize=(6, 4))
probplot(iio_log_r_tf, dist='norm', plot=plt)
plt.title('Q-Q Plot - iio Log Returns')
plt.xlabel('Theoretical Quantiles')
plt.ylabel('Sample Quantiles')
plt.grid(True)
plt.savefig("plots/Q-Q Plot - iio Log Returns (WGAN).svg", format="svg")
plt.close()

"""##### Monitoring the Training process
Apart from the qualitative metrics, we also need metrics of the ***quantitative*** kind in order to be able to compare different models with different parameters, as well as monitor the training process. The first one was introduced earlier as the Wasserstein-1 distance or EMD. Also, we need a way to quantify the stylized facts that depend on the timeframe. The *root mean squared error (RMSE)* is a frequently used measure of the differences between values, so we will exploit this.

For the autocorrelation of the log-returns, we can define the following cost function

$$\text{RMSE}(\rho^{S\&P}(\tau), \rho^{\theta}(\tau)) = \left ( \frac{1}{\tau_{max}} \sum_{\tau = 1}^{\tau_{max}} \left (\rho^{S\&P}(\tau) - \rho^{\theta}(\tau) \right )^2 \right )^{\frac{1}{2}}$$
where $\rho^{S\&P}(\tau)$ is the ACF of the log-returns of the S\&P 500, $\rho^{\theta,i}(\tau) $ is the ACF of the log-returns of the generated data of the model with parameters $\theta$ and the others are implied.

With the same approach, we can define a cost function for the absolute log-returns (volatility clustering) as

$$ \text{RMSE}(\rho^{S\&P} _{abs}(\tau), \rho^{\theta} _{abs}(\tau)) = \left ( \frac{1}{\tau_{max}} \sum_{\tau = 1}^{\tau_{max}} \left (\rho^{S\&P} _{abs}(\tau) - \rho^{\theta} _{abs}(\tau) \right )^2 \right )^{\frac{1}{2}} $$

where $\rho^{S\&P} _{abs}(\tau)$ is the ACF of the absolute log-returns of the original data and $\rho^{\theta} _{abs}(\tau)$ is the ACF of the absolute log-returns of the generated data from the model with parameters $\theta$.

Last, we define a cost function for the leverage effect as

$$ \text{RMSE}(L^{S\&P}(\tau), L^{\theta}(\tau)) = \left ( \frac{1}{\tau_{max}} \sum_{\tau = 1}^{\tau_{max}} \left (L^{S\&P}(\tau) - L^{\theta}(\tau) \right )^2 \right )^{\frac{1}{2}} $$

where the variables are implied.

Note that when we calculate cost functions when training neural networks, it is a common practice to evaluate the result over multiple samples. So, we should bear in mind to calculate the costs above over multiple generated time series.

### Model Implementation

The design of the WGAN with gradient penalty is based on *'Improved Training of Wasserstein GANs'* from *Ishaan Gulrajani* et.al: https://arxiv.org/abs/1704.00028

### Data Pre-processing

#### (1) Normalization

Now that we have all the information we need, we move to the design of the WGAN-GP. First, we will do some preprocessing on the financial time series data that we have (iio).

Let us normalize the data to have zero mean and unit variance. This helps the network learn more effectively by putting all features on a similar scale. It is a common standardization technique used in machine learning models. We can do this by the equation below

$$ r_t ^{\text{norm}} = \frac{r_t - \mu}{\sigma} $$

where $r_t ^{\text{norm}}$ is the normalized log-returns, $r_t$ the original log-returns, $\mu$ and $\sigma$ the mean and standard deviation of $r_t$ respectively.
"""

def normalize(data):
    mu = tf.reduce_mean(data)
    std = tf.math.reduce_std(data)

    return (data - mu)/std

# normalize the log-returns
iio_norm_r = normalize(iio_log_r_tf)
# display the mean and standard deviation of the original log-returns
print(f'Original iio log-returns mean = {tf.reduce_mean(iio_log_r_tf)}, std = {tf.math.reduce_std(iio_log_r_tf)}')
# display the mean and standard deviation of the normalized log-returns
print(f'Normalized iio log-returns mean = {tf.reduce_mean(iio_norm_r)}, std = {tf.math.reduce_std(iio_norm_r)}')

print('Original Data Min-Max')
print(tf.reduce_min(iio_log_r_tf).numpy(), tf.reduce_max(iio_log_r_tf).numpy())

print('Normalized Data Min-Max')
print(tf.reduce_min(iio_norm_r).numpy(), tf.reduce_max(iio_norm_r).numpy())

def denormalize(norm_data, mu_original, std_original):
    return norm_data*std_original + mu_original

fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(7,4))

# density of log-returns
bin_edges = np.linspace(-6, 6, num=100)  # define the bin edges
bin_width = bin_edges[1] - bin_edges[0]
bin_edges = np.append(bin_edges, bin_edges[-1] + bin_width)
axes.hist(iio_norm_r, bins=bin_edges, density=True, width=0.1, label='Normalized density')
axes.grid()

# normal distribution with same mean and standard deviation as log-returns
mu = np.mean(iio_norm_r)
sigma = np.std(iio_norm_r)

# Generate a set of points x
x = np.linspace(-6, 6, 100)

# Generate the Gaussian PDF for the points x with same mean and standard deviation as the log-returns
pdf = norm.pdf(x, mu, sigma)

# plot the Gaussian PDF
axes.plot(x, pdf, 'r', label='Gaussian')
axes.legend()

plt.savefig("plots/Gaussian_PDF_and_Normalized_density (WGAN).svg", format="svg")
plt.close()

"""#### (2) Inverse Lambert W Transform

Given the limited number of samples in the tails, learning a heavy-tailed distribution can be difficult. To address this challenge, we can employ a method to decrease the influence of these tails. One approach involves applying an inverse Lambert-W transform to the normalized log returns. By applying this transformation, we can convert the heavy-tailed data into a distribution that closely resembles a Gaussian distribution. This can be advantageous for further analysis or modeling tasks that assume Gaussianity.

Given Lambert's W function which is the inverse of $ z = u e^u$ with $z : \mathbb{R} \rightarrow \mathbb{R}$, we can define the following transform to our normalized heavy-tailed data set $V$ as

$$ W_{\delta}(v) = sgn(v) \sqrt{\frac{W(\delta v^2)}{\delta}} $$

where $v \in V$, $\delta \geq 0$ a tunable parameter and $sgn(v)$ the sign of $v$ and $W$ the Lambert function.

The Gaussianized data can be transformed back to its original state using the equation

$$ v = W_{\delta}(v) \exp \left ( \frac{\delta}{2} W^2 _{\delta}(v) \right ) $$
"""

def inverse_lambert_w_transform(data, delta):
    """
    Apply inverse Lambert W transform to the input data using the specified delta value.

    Parameters:
    - data: Input data tensor
    - delta: Delta value for the transform (tail parameter)

    Returns:
    - Transformed data array
    """
    sign = tf.sign(tf.cast(data, dtype=tf.float64))
    transformed_data = sign * tf.cast(tf.sqrt(lambertw(delta * data ** 2).real / delta), dtype=tf.float64)

    return transformed_data

def lambert_w_transform(transformed_data, delta, clip_low=-12.0, clip_high=11.0):
    """
    Transform the Gaussianized data back to its original state.

    Parameters:
    - transformed_data: Input data array which was transformed using inverse Lambert W
    - delta: Delta value for the transform (tail parameter)

    Returns:
    - Original Data
    """
    reversed_data = transformed_data*tf.cast(tf.exp((delta/2) * transformed_data ** 2), dtype=tf.float64)
    return tf.clip_by_value(reversed_data, clip_low, clip_high)

# apply inverse Lambert W transform to the normalized log-returns
transformed_iio = inverse_lambert_w_transform(iio_norm_r, 1)

fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(7,4))

# density of normalized log-returns
bin_edges = np.linspace(-3, 3, num=50)  # define the bin edges
bin_width = bin_edges[1] - bin_edges[0]
bin_edges = np.append(bin_edges, bin_edges[-1] + bin_width)
axes.hist(transformed_iio, bins=bin_edges, density=True, width=0.1, label='Transformed log-returns')
axes.grid()

# normal distribution with same mean and standard deviation as log-returns
mu = np.mean(transformed_iio)
sigma = np.std(transformed_iio)

# Generate a set of points x
x = np.linspace(-3, 3, 100)

# Generate the Gaussian PDF for the points x with same mean and standard deviation as the normalized log-returns
pdf = norm.pdf(x, mu, sigma)

# plot the Gaussian PDF
axes.plot(x, pdf, 'r', label='Gaussian')
axes.legend()

plt.savefig("plots/Gaussian_PDF_and_Transformed_log_returns (WGAN).svg", format="svg")
plt.close()

# Generate the Q-Q plot
plt.figure(figsize=(6, 4))
probplot(transformed_iio, dist='norm', plot=plt)
plt.title('Q-Q Plot - Transformed iio Log Returns')
plt.xlabel('Theoretical Quantiles')
plt.ylabel('Sample Quantiles')
plt.grid(True)
plt.savefig("plots/Q-Q Plot - Transformed iio Log Returns (WGAN).svg", format="svg")
plt.close()

print('Transformed Data Min-Max')
print(tf.reduce_min(transformed_iio).numpy(), tf.reduce_max(transformed_iio).numpy())

print('Transformed data mean: ', tf.reduce_mean(transformed_iio).numpy())
print('Transformed data std: ', tf.math.reduce_std(transformed_iio).numpy())

"""It is clear that our transformed data matches a normal distribution quite well.

#### (3) Normalization
"""

# normalize the transformed data
transformed_iio_norm = normalize(transformed_iio)
print(f'Normalized Transformed log-returns mean = {tf.reduce_mean(transformed_iio_norm)}, std = {tf.math.reduce_std(transformed_iio_norm)}')

print('Normalized Transformed Data Min-Max')
print(tf.reduce_min(transformed_iio_norm).numpy(), tf.reduce_max(transformed_iio_norm).numpy())

"""### Check Reverse Process

#### (3) Denormalize
"""

# denormalize the transformed data
transformed_iio_denorm = denormalize(transformed_iio_norm, tf.reduce_mean(transformed_iio), tf.math.reduce_std(transformed_iio))
print('Transformed (denormalized) data mean: ', tf.reduce_mean(transformed_iio_denorm).numpy())
print('Transformed (denormalized) data std: ', tf.math.reduce_std(transformed_iio_denorm).numpy())

fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(7,4))

# density of normalized log-returns
bin_edges = np.linspace(-3, 3, num=50)  # define the bin edges
bin_width = bin_edges[1] - bin_edges[0]
bin_edges = np.append(bin_edges, bin_edges[-1] + bin_width)
axes.hist(transformed_iio_denorm, bins=bin_edges, density=True, width=0.1, label='Transformed log-returns')
axes.grid()

# normal distribution with same mean and standard deviation as log-returns
mu = np.mean(transformed_iio_denorm)
sigma = np.std(transformed_iio_denorm)

# Generate a set of points x
x = np.linspace(-3, 3, 100)

# Generate the Gaussian PDF for the points x with same mean and standard deviation as the normalized log-returns
pdf = norm.pdf(x, mu, sigma)

# plot the Gaussian PDF
axes.plot(x, pdf, 'r', label='Gaussian')
axes.legend()

plt.savefig("plots/Reverse_Gaussian_PDF_and_Transformed_log_returns (WGAN).svg", format="svg")
plt.close()

"""#### (2) Lambert W Transform"""

iio_norm_r_reversed = lambert_w_transform(transformed_iio_denorm, 1)

fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(7,4))

# density of log-returns
bin_edges = np.linspace(-6, 6, num=100)  # define the bin edges
bin_width = bin_edges[1] - bin_edges[0]
bin_edges = np.append(bin_edges, bin_edges[-1] + bin_width)
axes.hist(iio_norm_r_reversed, bins=bin_edges, density=True, width=0.1, label='Normalized density')
axes.grid()

# normal distribution with same mean and standard deviation as log-returns
mu = np.mean(iio_norm_r_reversed)
sigma = np.std(iio_norm_r_reversed)

# Generate a set of points x
x = np.linspace(-6, 6, 100)

# Generate the Gaussian PDF for the points x with same mean and standard deviation as the log-returns
pdf = norm.pdf(x, mu, sigma)

# plot the Gaussian PDF
axes.plot(x, pdf, 'r', label='Gaussian')
axes.legend()

plt.savefig("plots/Reverse_Gaussian_PDF_and_Normalized_density (WGAN).svg", format="svg")
plt.close()

print('Denormalized data mean: ', tf.reduce_mean(iio_norm_r_reversed).numpy())
print('Denormalized data std: ', tf.math.reduce_std(iio_norm_r_reversed).numpy())

print('Denormalized Data Min-Max')
print(tf.reduce_min(iio_norm_r_reversed).numpy(), tf.reduce_max(iio_norm_r_reversed).numpy())

"""#### (1) Denormalize"""

# denormalize the initial data
transformed_iio_denorm = denormalize(iio_norm_r_reversed, tf.reduce_mean(iio_log_r_tf), tf.math.reduce_std(iio_log_r_tf))
print('Original (denormalized) data mean: ', tf.reduce_mean(transformed_iio_denorm).numpy())
print('Original (denormalized) data std: ', tf.math.reduce_std(transformed_iio_denorm).numpy())

print('Original Data Min-Max')
print(tf.reduce_min(transformed_iio_denorm).numpy(), tf.reduce_max(transformed_iio_denorm).numpy())

# Generate the Q-Q plot
plt.figure(figsize=(6, 4))
probplot(transformed_iio_denorm, dist='norm', plot=plt)
plt.title('Q-Q Plot - Reversed pre-processed iio Log Returns')
plt.xlabel('Theoretical Quantiles')
plt.ylabel('Sample Quantiles')
plt.grid(True)
plt.savefig("plots/Q-Q Plot - Reversed pre-processed iio Log Returns (WGAN).svg", format="svg")
plt.close()

"""The reverse process works as expected. We can now move on to applying the rolling window technique.

#### (4) Rolling Window

The rolling window technique is commonly used in time series analysis to generate multiple subsequences from a given time series. It involves defining a window of length $m$ and a stride $s$. The window length determines the size of each subsequence, while the stride determines the distance between the start of one subsequence and the start of the next. By applying the rolling window with the specified window length and stride to the time series, we obtain a set of subsequences each of which can be treated as a new sample in our dataset. If we assume that there are $n$ total samples, then after applying the rolling window we get $ \lfloor{\frac{n-m}{s}}\rfloor + 1 $ samples, which will be fed to the model at each training epoch.


As we may notice, there is often overlap between adjacent subsequences due to the stride being shorter than the window length. This can lead to correlation between training samples, which is not ideal. However, the larger number of training samples generated by the rolling window technique can be beneficial for model performance, as it provides more data for the model to learn from.

The following function implements a rolling window to the one-dimensional input data and returns a dataset (tensor) of $ \lfloor{\frac{n-m}{s}}\rfloor + 1 $ rows and $m$ columns:
"""

def rolling_window(data, m, s):
    return tf.map_fn(lambda i: data[i:i+m], tf.range(0, len(data) - m + 1, s), dtype=tf.float64)

"""### Critic & Generator Models

#### Critic Model

Research has been conducted on how the critic should be efficiently implemented in a WGAN-GP and it has been shown that *convolutional neural networks (CNNs)* with one-dimensional convolutional layers are able to produce financial time series with stylized facts, rather than using a simple feed-forward network. For further details, refer to Chapter 3 of this thesis: *Schwander, E. (2022). Quantum generative modelling for financial time series*, which is further based in *'Improved Training of Wasserstein GANs'* from *Ishaan Gulrajani* et.al: https://arxiv.org/abs/1704.00028

So, we will follow this approach as our main concern is not the design of a classical generative model for financial time series, but a hybrid variation using quantum computation.

In this implementation, we define a function that takes a window length as input and returns the critic model. The model has three 1D convolutional layers with 64, 128, and 128 filters respectively. Each convolutional layer has a kernel size of 3 and a stride of 1 and is followed by a leakyReLU activation function with an alpha value of 0.01. In a CNN architecture, the convolutional layers are typically followed by one or more dense layers to process the extracted features and make predictions. The Flatten layer is to reshape the output of the convolutional layers into a 1-dimensional tensor, which is then fed into a dense layer of 32 neurons and leakyReLu activation and a dropout layer with a rate of 0.2 for introducing better generalization. The final layer is a dense layer with a single output, which is linear. Note that one should not confuse the 'strides' in this code with the window stride, as the first determines the step size of the sliding window in the convolution operation.

#### Generator Model

Here, we also define a simple feedforward neural network using the Sequential API of TensorFlow. The *latent_dim* is the dimensionality of the input noise vector that will be used to generate fake samples.

A linear activation for the generator output is reasonable, as the critic gives feedback based on the preprocessed samples. This means that the generator tries to capture properties from the preprocessed data, hence for proper evaluation, the generator's output should be processed in a reverse manner to form the original log-returns.

### The WGAN-GP

The WGAN-GP model is designed based on *I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville,
Improved Training of Wasserstein GANs*. In contrast to the original GAN formulation, the authors have made some adjustments.

 - During each WGAN training iteration, the generator is updated once for every *n_critic* updates of the critic, where *n_critic* is a hyperparameter that determines how many times the critic is trained before the generator is updated. This is to ensure that the critic is able to distinguish real samples from generated samples with high confidence and that it is able to provide meaningful feedback to the generator. This is important because the generator's updates are based on the critic's feedback, so if the critic is not properly trained, the generator's updates may not be effective.
 - Interpolation is used to encourage the generated samples to lie along the straight line between the real and generated samples. By computing the interpolated samples as a convex combination of the real and generated samples using a uniform random epsilon value, the generator is encouraged to produce samples that are more similar to the real samples and lie along the data distribution manifold. This helps to avoid the problem of gradient vanishing and gradient explosion that can occur during training when the gradient of the critic with respect to the generator becomes zero or infinite respectively.
 - The authors propose a different loss function for the generator called the "negative critic loss" or "minimax loss" instead of the binary cross-entropy loss used in traditional GANs. The idea is to maximize the critic's output for fake samples, which is equivalent to minimizing the negative critic loss. This is different from traditional GANs, where the generator tries to maximize the log probability of the discriminator being wrong.
"""

class WGAN_GP(tf.keras.Model):
    def __init__(self, num_epochs, batch_size, latent_dim, window_size, n_critic, gp):
        super(WGAN_GP, self).__init__()
        # define the critic and generator networks
        self.critic = self.define_critic_model(window_size)
        self.generator = self.define_generator_model(latent_dim, window_size)
        self.num_epochs = num_epochs
        self.batch_size = batch_size
        self.latent_dim = latent_dim
        self.window_size = window_size
        self.n_critic = n_critic
        self.gp = gp
        # average critic and generator losses for each epoch
        self.critic_loss_avg = []
        self.generator_loss_avg = []
        # Earth's mover distance (EMD) for each epoch
        self.emd_avg = []
        # stylized facts RMSEs for each epoch
        self.acf_avg = []
        self.vol_avg = []
        self.lev_avg = []

    def define_critic_model(self, window_length):
        model = tf.keras.Sequential()
        model.add(tf.keras.layers.Conv1D(filters=64, kernel_size=10, strides=1, input_shape=(window_length, 1), padding='same'))
        model.add(tf.keras.layers.LeakyReLU(alpha=0.1))

        model.add(tf.keras.layers.Conv1D(filters=128, kernel_size=10, strides=1, padding='same'))
        model.add(tf.keras.layers.LeakyReLU(alpha=0.1))

        model.add(tf.keras.layers.Conv1D(filters=128, kernel_size=10, strides=1, padding='same'))
        model.add(tf.keras.layers.LeakyReLU(alpha=0.1))

        model.add(tf.keras.layers.Flatten())

        model.add(tf.keras.layers.Dense(32, dtype=tf.float64))
        model.add(tf.keras.layers.LeakyReLU(alpha=0.1))
        model.add(tf.keras.layers.Dropout(0.2))

        model.add(tf.keras.layers.Dense(1, dtype=tf.float64))

        return model

    def define_generator_model(self, latent_dim, window_length):
        model = tf.keras.Sequential()
        model.add(tf.keras.layers.Dense(30, input_shape=(latent_dim,)))
        model.add(tf.keras.layers.LeakyReLU(alpha=0.1))

        model.add(tf.keras.layers.Dense(50))
        model.add(tf.keras.layers.LeakyReLU(alpha=0.1))

        model.add(tf.keras.layers.Dense(50))
        model.add(tf.keras.layers.LeakyReLU(alpha=0.1))
        model.add(tf.keras.layers.Dropout(0.2))

        model.add(tf.keras.layers.Dense(50))
        model.add(tf.keras.layers.LeakyReLU(alpha=0.1))
        model.add(tf.keras.layers.Dropout(0.2))

        model.add(tf.keras.layers.Dense(window_length, dtype=tf.float64))

        return model

    # compile model with given optimizers for critic and generator networks
    def compile_WGAN(self, c_optimizer, g_optimizer):
        super(WGAN_GP, self).compile()
        self.c_optimizer = c_optimizer
        self.g_optimizer = g_optimizer

    def train_wgan_gp(self, gan_data, original_data, preprocessed_data, num_elements):
        """
        Parameters:
         - gan_data is the preprocessed dataset with windows for WGAN training
         - original_data is the original iio log-returns for evaluation of RMSEs (monitoring purposes)
         - preprocessed_data is the preprocessed log-returns without the last normalization step and without windows
          (for reversing the process of generated samples using the mean and std and evaluating the RMSEs)
        """
        for epoch in range(self.num_epochs):
            print(f'Processing epoch {epoch+1}/{self.num_epochs}')
            ################################################################
            #
            # Train the critic for n_critic iterations
            # Process 'batch_size' samples in each iteration independently
            #
            ################################################################
            # critic loss for 'n_critic' iterations
            critic_t_sum = 0
            for t in range(self.n_critic):
                # record the gradients
                with tf.GradientTape() as critic_tape:
                    # critic loss for 'batch_size' samples
                    critic_sum = 0
                    for i in range(self.batch_size):
                        # shuffle the dataset
                        shuffled_data = gan_data.shuffle(buffer_size=num_elements)
                        # take a single random element from the shuffled dataset
                        random_element = shuffled_data.take(1)
                        # iterate over the random_element dataset to access the value
                        for element in random_element:
                            # access the value of the random element as a tensor
                            real_sample = element
                        # reshape the real sample for compatibility with the first layer of the critic
                        real_sample = tf.reshape(real_sample, (1, self.window_size))

                        # generate latent noise for the generator
                        latent_noise = tf.random.normal(shape=(1, self.latent_dim))

                        # generate fake samples using the generator
                        generated_sample = self.generator(latent_noise)

                        # calculate the critic scores for real and fake samples
                        real_score = self.critic(real_sample)
                        fake_score = self.critic(generated_sample)

                        # compute the gradient penalty
                        gradient_penalty = self.compute_gradient_penalty(real_sample, generated_sample)

                        # calculate the Wasserstein distance loss with gradient penalty
                        critic_loss = fake_score - real_score + self.gp * gradient_penalty
                        # accumulate the critic loss for the sample
                        critic_sum += critic_loss

                    # compute the gradients of critic and apply them
                    critic_gradients = critic_tape.gradient(critic_sum/self.batch_size, self.critic.trainable_variables)
                    self.c_optimizer.apply_gradients(zip(critic_gradients, self.critic.trainable_variables))

                # accumulate the average critic loss for all samples in this 't' iteration
                critic_t_sum += critic_sum/self.batch_size

            # average critic loss for this epoch of WGAN training
            self.critic_loss_avg.append(critic_t_sum/self.n_critic)

            ################################################################
            #
            # Train generator for one iteration
            #
            ################################################################
            # sample a batch of latent variables
            latent_noise = tf.random.normal(shape=(self.batch_size, self.latent_dim))
            with tf.GradientTape() as gen_tape:
                # generate fake samples using the generator
                generated_samples = self.generator(latent_noise)
                # calculate the critic scores for fake samples
                fake_scores = self.critic(generated_samples)
                # calculate the generator loss
                generator_loss = -tf.reduce_mean(fake_scores)

            # compute the gradients of generator and apply them
            generator_gradients = gen_tape.gradient(generator_loss, self.generator.trainable_variables)
            self.g_optimizer.apply_gradients(zip(generator_gradients, self.generator.trainable_variables))

            # average generator loss for this epoch
            self.generator_loss_avg.append(generator_loss)

            #######################################################################################################
            #
            # Calculate the stylized facts RMSEs and the EMD for real and fake data
            #
            # Fake data has shape (num_samples x window_size), where num_samples = original_length / window_size
            # in order to get a time series close to the length of the original
            #
            #######################################################################################################
            # generate noise
            num_samples = len(original_data) // self.window_size
            latent_noise = tf.random.normal(shape=(num_samples, self.latent_dim))
            # generate fake samples using the generator
            batch_generated = self.generator.predict(latent_noise, verbose=0)
            # concatenate all time series data into one
            generated_data = tf.reshape(batch_generated, shape=(num_samples*self.window_size,))

            # reverse the preprocessing on generated sample
            transformed_iio_denorm = denormalize(generated_data, tf.reduce_mean(preprocessed_data), tf.math.reduce_std(preprocessed_data))
            original_norm = lambert_w_transform(transformed_iio_denorm, 1)
            fake_original = denormalize(original_norm, tf.reduce_mean(original_data), tf.math.reduce_std(original_data))

            # calculate the temporal metrics for monitoring the training process
            corr_rmse, volatility_rmse, lev_rmse, emd = self.stylized_facts(original_data, fake_original)
            # store the EMD and RMSEs of stylized facts
            self.acf_avg.append(corr_rmse)
            self.vol_avg.append(volatility_rmse)
            self.lev_avg.append(lev_rmse)
            self.emd_avg.append(emd)

            # checkpoint saving
            if (epoch + 1) % 20 == 0:
                self.generator.save_weights(f"checkpoints/generator_epoch_WGAN_{epoch+1}.weights.h5")
                self.critic.save_weights(f"checkpoints/critic_epoch_WGAN_{epoch+1}.weights.h5")

            # print progress every 100 epochs
            if epoch % 100 == 0 or epoch+1 == 3000:
                print(f'\nEpoch {epoch+1} completed')
                print(f'Critic loss (average): {self.critic_loss_avg[epoch][-1][0]}')
                print(f'Generator loss (average): {self.generator_loss_avg[epoch]}')
                print(f'\nEMD (average): {self.emd_avg[epoch]}')
                print(f'ACF RMSE (average): {self.acf_avg[epoch]}')
                print(f'VOLATILITY RMSE (average): {self.vol_avg[epoch]}')
                print(f'LEVERAGE RMSE (average): {self.lev_avg[epoch]}\n')
                print('Min-Max values of original log-returns: ', tf.reduce_min(original_data).numpy(), tf.reduce_max(original_data).numpy())
                print('Min-Max values of generated log-returns (for all batches): ', tf.reduce_min(fake_original).numpy(), tf.reduce_max(fake_original).numpy())
                print('Min-Max values after Lambert: ', tf.reduce_min(original_norm).numpy(), tf.reduce_max(original_norm).numpy())
                print()

    ###########################################################
    #
    # Sample a random number epsilon ~ U[0,1]
    # Create a convex combination of real and generated sample
    # Compute the gradient penalty for the critic network
    #
    ###########################################################
    def compute_gradient_penalty(self, real_sample, generated_sample):
        epsilon = tf.random.uniform((), dtype=tf.float64)
        interpolated_sample = epsilon * real_sample + (1 - epsilon) * generated_sample

        with tf.GradientTape() as tape:
            tape.watch(interpolated_sample)
            scores = self.critic(interpolated_sample)

        gradients = tape.gradient(scores, interpolated_sample)
        gradients_norm = tf.norm(gradients)
        gradient_penalty = (gradients_norm - 1)**2

        return gradient_penalty

    def stylized_facts(self, original_data, fake_original):
        """
        - Calculate the RMSEs of the stylized facts between the original iio log-returns and generated

        - Evaluate the EMD between real and generated samples (reversed preprocessing)
        """

        ################################################
        #
        # stylized facts for fake samples
        #
        ################################################

        # compute acf for maximum lags = 28
        acf_values = sm.tsa.acf(fake_original, nlags=28)
        # exclude zero lag
        acf_values_generated = tf.convert_to_tensor(acf_values[1:])

        # compute absolute acf (volatility clustering) for maximum lags = 28
        acf_abs_values = sm.tsa.acf(tf.abs(fake_original), nlags=28)
        # exclude zero lag
        acf_abs_values_generated = tf.convert_to_tensor(acf_abs_values[1:])

        # compute leverage effect for maximum lags = 28
        lev = []
        for lag in range(1, 29):
            # slice the tensors to get the appropriate lagged sequences
            r_t = fake_original[:-lag]
            squared_lag_r = tf.square(tf.abs(fake_original[lag:]))

            # calculate the leverage effect
            lev.append(tfp.stats.correlation(r_t, squared_lag_r, sample_axis=0, event_axis=None))

        leverage_generated = tf.convert_to_tensor(lev)

        ################################################
        #
        # stylized facts for real samples
        #
        ################################################

        # compute acf for maximum lags = 28
        acf_values = sm.tsa.acf(original_data, nlags=28)
        # exclude zero lag
        acf_values_original = tf.convert_to_tensor(acf_values[1:])

        # compute absolute acf (volatility clustering) for maximum lags = 28
        acf_abs_values = sm.tsa.acf(tf.abs(original_data), nlags=28)
        # exclude zero lag
        acf_abs_values_original = tf.convert_to_tensor(acf_abs_values[1:])

        # compute leverage effect for maximum lags = 28
        lev = []
        for lag in range(1, 29):
            # slice the tensors to get the appropriate lagged sequences
            r_t = original_data[:-lag]
            squared_lag_r = tf.square(tf.abs(original_data[lag:]))

            # calculate the leverage effect
            lev.append(tfp.stats.correlation(r_t, squared_lag_r, sample_axis=0, event_axis=None))

        leverage_original = tf.convert_to_tensor(lev)

        # calculate average RMSEs of stylized facts
        # autocorrelations
        rmse_acf = tf.sqrt(tf.reduce_mean((acf_values_original-acf_values_generated)**2))
        # volatility clustering
        rmse_vol = tf.sqrt(tf.reduce_mean((acf_abs_values_original-acf_abs_values_generated)**2))
        # leverage effect
        rmse_lev = tf.sqrt(tf.reduce_mean((leverage_original-leverage_generated)**2))

        ####################################################################################
        #
        # compute the Earth's mover distance (EMD)
        #
        ####################################################################################
        bin_edges = np.linspace(-1, 1, num=100)  # define the bin edges
        bin_width = bin_edges[1] - bin_edges[0]
        bin_edges = np.append(bin_edges, bin_edges[-1] + bin_width)
        # compute the empirical distribution of original data
        empirical_real, _ = np.histogram(original_data, bins=bin_edges, density=True)
        empirical_real /= np.sum(empirical_real)
        # compute the empirical distribution of generated data
        empirical_fake, _ = np.histogram(fake_original, bins=bin_edges, density=True)
        empirical_fake /= np.sum(empirical_fake)

        # evaluate the EMD using SciPy
        emd = wasserstein_distance(empirical_real, empirical_fake)

        return rmse_acf, rmse_vol, rmse_lev, emd

"""At last, we are ready to define the model hyperparameters and move to training:"""

##################################################################
#
# Hyperparameters
#
##################################################################
LATENT_DIM = 8
WINDOW_SIZE = 30

# training hyperparameters
EPOCHS = 1000 #3000
BATCH_SIZE = 32

n_critic = 5 # number of iterations for the critic per epoch
LAMBDA = 10  # gradient penalty strength

# instantiate the model object
gan = WGAN_GP(EPOCHS, BATCH_SIZE, LATENT_DIM, WINDOW_SIZE, n_critic, LAMBDA)

# set the optimizer along with the learning rate and beta parameters (default)
c_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)
g_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)
gan.compile_WGAN(c_optimizer, g_optimizer)

##################################################################################
#
# Data pre-processing
#
##################################################################################
# apply rolling window in transformed normalized log-returns with stride s=1
gan_data_tf = rolling_window(transformed_iio_norm, WINDOW_SIZE, 1)
# create TensorFlow datasets
gan_data = tf.data.Dataset.from_tensor_slices(gan_data_tf)
# get the number of elements in the dataset
num_elements = gan_data.cardinality().numpy()

# train the WGAN
print('Training started...')
print('Number of samples to process per epoch: ', num_elements)
print()
start_time_train = time.time()
gan.train_wgan_gp(gan_data, iio_log_r_tf, transformed_iio, num_elements)
exec_time_train = time.time() - start_time_train
print(f'\nWGAN training completed. Training time: --- {exec_time_train/3600:.02f} hours ---')

# code to SAVE qgan arrays in a file like emd_avg , acf etc

import numpy as np

# Create a dictionary to store the arrays
wgan_metrics = {
    'emd_avg': gan.emd_avg,
    'acf_avg': gan.acf_avg,
    'vol_avg': gan.vol_avg,
    'lev_avg': gan.lev_avg,
    'generator_loss_avg': gan.generator_loss_avg,
    'critic_loss_avg': gan.critic_loss_avg
}

# Save the dictionary to a file
np.save('wgan_metrics.npy', wgan_metrics)

print("WGAN metrics saved to wgan_metrics.npy")

"""## Plot Training History"""

critic_loss = tf.squeeze(gan.critic_loss_avg, axis=(1,2)).numpy()
generator_loss = np.array(gan.generator_loss_avg)

window = 50
generator_ma = np.convolve(generator_loss, np.ones(window)/window, mode='valid')
critic_ma = np.convolve(critic_loss, np.ones(window)/window, mode='valid')

# plot the graphs side-by-side
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,4))

# plot the critic loss moving average as a line
axes[0].plot(range(window-1, len(critic_loss)), critic_ma, label='Average Critic Loss', color='blue')
# plot the critic loss
axes[0].plot(critic_loss, color='black', alpha=0.2)

# plot the generator loss moving average as a line
axes[0].plot(range(window-1, len(generator_loss)), generator_ma, label='Average Generator Loss', color='orange')
# plot the generator loss
axes[0].plot(generator_loss, color='black', alpha=0.2)


axes[0].set_ylabel('Loss')
axes[0].legend()
axes[0].grid()

emd_avg = np.array(gan.emd_avg)
emd_ma = np.convolve(emd_avg, np.ones(window)/window, mode='valid')

axes[1].plot(range(window-1, len(emd_avg)), emd_ma, label='EMD', color='red')
axes[1].plot(emd_avg, color='red', linewidth=0.5, alpha=0.5)

axes[1].set_ylabel('EMD')
axes[1].legend()
axes[1].grid()

acf_avg = np.array(gan.acf_avg)
vol_avg = np.array(gan.vol_avg)
lev_avg = np.array(gan.lev_avg)

acf_ma = np.convolve(acf_avg, np.ones(window)/window, mode='valid')
vol_ma = np.convolve(vol_avg, np.ones(window)/window, mode='valid')
lev_ma = np.convolve(lev_avg, np.ones(window)/window, mode='valid')

# Creating a twin axes for the second graph
axes2 = axes[1].twinx()

axes2.plot(range(window-1, len(acf_avg)), acf_ma, label='ACF', color='green')
axes2.plot(acf_avg, color='green', linewidth=0.5, alpha=0.4)

axes2.plot(range(window-1, len(vol_avg)), vol_ma, label='Volatility Clustering', color='black')
axes2.plot(vol_avg, color='black', linewidth=0.5, alpha=0.3)

axes2.plot(range(window-1, len(lev_avg)), lev_ma, label='Leverage Effect', color='orange')

axes2.set_ylabel('Temporal Metrics')
axes2.legend()
axes2.grid()

# Adjusting the spacing between subplots
plt.tight_layout()
plt.savefig("plots/training_history (WGAN).svg", format="svg")
plt.close()

critic_loss = tf.squeeze(gan.critic_loss_avg, axis=(1,2)).numpy()
generator_loss = np.array(gan.generator_loss_avg)

window = 50
generator_ma = np.convolve(generator_loss, np.ones(window)/window, mode='valid')
critic_ma = np.convolve(critic_loss, np.ones(window)/window, mode='valid')

# plot the graphs side-by-side
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,4))

# plot the critic loss moving average as a line
axes[0].plot(range(window-1, len(critic_loss)), critic_ma, label='Average Critic Loss', color='blue')
# plot the critic loss
axes[0].plot(critic_loss, color='black', alpha=0.2)

# plot the generator loss moving average as a line
axes[0].plot(range(window-1, len(generator_loss)), generator_ma, label='Average Generator Loss', color='orange')
# plot the generator loss
axes[0].plot(generator_loss, color='black', alpha=0.2)


axes[0].set_ylabel('Loss')
axes[0].legend()
axes[0].grid()

emd_avg = np.array(gan.emd_avg)
emd_ma = np.convolve(emd_avg, np.ones(window)/window, mode='valid')

axes[1].plot(range(window-1, len(emd_avg)), emd_ma, label='EMD', color='red')
axes[1].plot(emd_avg, color='red', linewidth=0.5, alpha=0.5)

axes[1].set_ylabel('EMD')
axes[1].legend()
axes[1].grid()

axes2.set_ylabel('Temporal Metrics')
axes2.legend()
axes2.grid()

# Adjusting the spacing between subplots
plt.tight_layout()
plt.savefig("plots/training_history_sliced(WGAN).svg", format="svg")
plt.close()

"""## Plot Generated Data Properties"""

num_samples = len(iio_log_r_tf) // WINDOW_SIZE
latent_noise = tf.random.normal(shape=(num_samples, LATENT_DIM))
# generate data using the generator
generated_data = gan.generator.predict(latent_noise)
generated_data = tf.reshape(generated_data, shape=(num_samples*WINDOW_SIZE,))

# reverse the preprocessing on generated series
transformed_iio_denorm = denormalize(generated_data, tf.reduce_mean(transformed_iio), tf.math.reduce_std(transformed_iio))
original_norm = lambert_w_transform(transformed_iio_denorm, 1)
fake_original = denormalize(original_norm, tf.reduce_mean(iio_log_r_tf), tf.math.reduce_std(iio_log_r_tf))

print('Min-Max values of generated data: ', tf.reduce_min(generated_data).numpy(), tf.reduce_max(generated_data).numpy())
print('Min-Max values of generated log-returns (for all batches): ', tf.reduce_min(fake_original).numpy(), tf.reduce_max(fake_original).numpy())
print('Min-Max values after Lambert: ', tf.reduce_min(original_norm).numpy(), tf.reduce_max(original_norm).numpy())

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,4))
###################################################################################################################
#
# plot original log-returns on the left and the generated on the right
#
###################################################################################################################
axes[0].plot(date[1:], iio_log_r_tf)
axes[0].set_xlabel('Days')
axes[0].set_title('Original Log-Returns')
axes[0].grid()

axes[1].plot(fake_original)
axes[1].set_xlabel('Days')
axes[1].set_title('Generated Log-Returns')
axes[1].grid()

plt.savefig("plots/Original Log-Returns_and_Generated Log-Returns (WGAN).svg", format="svg")
plt.close()

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,4))
###################################################################################################################
#
# plot histogram of generated along with original log-returns on the left and the Q-Q plot on the right
#
###################################################################################################################
bin_edges = np.linspace(-0.05, 0.05, num=50)  # define the bin edges
bin_width = bin_edges[1] - bin_edges[0]
bin_edges = np.append(bin_edges, bin_edges[-1] + bin_width)
axes[0].hist(fake_original, bins=bin_edges, density=True, width=0.001, label='Generated', alpha=0.9)
axes[0].hist(iio_log_r_tf, bins=bin_edges, density=True, width=0.001, label='Original', alpha=0.8)
axes[0].set_title('Original vs Generated Density')
axes[0].grid()
axes[0].legend()

probplot(fake_original, dist='norm', plot=axes[1])
axes[1].set_xlabel('Theoretical Quantiles')
axes[1].grid()

plt.subplots_adjust(wspace=0.3)

plt.savefig("plots/Original vs Generated Density (WGAN).svg", format="svg")
plt.close()

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,4))
###################################################################################################################
#
# plot autocorrelations of original log-returns on the left and the generated on the right
#
###################################################################################################################
tsaplots.plot_acf(iio_log_r_tf, ax=axes[0], lags=28, zero=False)
axes[0].set_xlabel('Lags')
axes[0].set_title('ACF Log-Returns')
axes[0].grid()

tsaplots.plot_acf(fake_original, ax=axes[1], lags=28, zero=False)
axes[1].set_xlabel('Lags')
axes[1].set_title('ACF Log-Returns (Generated)')
axes[1].grid()

plt.savefig("plots/ACF Log-Returns (Generated) (WGAN).svg", format="svg")
plt.close()

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,4))
###################################################################################################################
#
# plot volatility clustering of original log-returns on the left and the generated on the right
#
###################################################################################################################
tsaplots.plot_acf(tf.abs(iio_log_r_tf), ax=axes[0], lags=28, zero=False)
axes[0].set_xlabel('Lags')
axes[0].set_title('ACF Absolute Log-Returns')
axes[0].grid()

tsaplots.plot_acf(tf.abs(fake_original), ax=axes[1], lags=28, zero=False)
axes[1].set_xlabel('Lags')
axes[1].set_title('ACF Absolute Log-Returns (Generated)')
axes[1].grid()
plt.savefig("plots/ACF Absolute Log-Returns (Generated) (WGAN).svg", format="svg")
plt.close()

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,4))
###################################################################################################################
#
# plot leverage effect of original log-returns on the left and the generated on the right
#
###################################################################################################################
# compute leverage effect for maximum lags = 28
leverage_original = []
for lag in range(1, 29):
    # slice the tensors to get the appropriate lagged sequences
    r_t = iio_log_r_tf[:-lag]
    squared_lag_r = tf.square(tf.abs(iio_log_r_tf[lag:]))

    # calculate the leverage effect
    leverage_original.append(tfp.stats.correlation(r_t, squared_lag_r, sample_axis=0, event_axis=None))

leverage_generated = []
for lag in range(1, 29):
    # slice the tensors to get the appropriate lagged sequences
    r_t = fake_original[:-lag]
    squared_lag_r = tf.square(tf.abs(fake_original[lag:]))

    # calculate the leverage effect
    leverage_generated.append(tfp.stats.correlation(r_t, squared_lag_r, sample_axis=0, event_axis=None))

axes[0].plot(leverage_original)
axes[0].set_xlabel('Lags')
axes[0].set_title('Original Leverage Effect')
axes[0].grid()

axes[1].plot(leverage_generated)
axes[1].set_xlabel('Lags')
axes[1].set_title('Generated Leverage Effect')
axes[1].grid()
plt.savefig("plots/Generated Leverage Effect (WGAN).svg", format="svg")
plt.close()

gan.generator.summary()

gan.critic.summary()

